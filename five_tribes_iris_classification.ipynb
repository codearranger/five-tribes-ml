{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Tribes of Machine Learning: Iris Classification\n",
    "## An Educational Demonstration\n",
    "\n",
    "Welcome! This notebook demonstrates how the five tribes of machine learning (from Pedro Domingos' book \"The Master Algorithm\") each approach the classic Iris flower classification problem.\n",
    "\n",
    "### The Five Tribes:\n",
    "- üå≥ **Symbolists** - Learn through logical rules\n",
    "- üß† **Connectionists** - Learn by mimicking the brain\n",
    "- üß¨ **Evolutionaries** - Learn through simulated evolution\n",
    "- üìä **Bayesians** - Learn through probabilistic inference\n",
    "- üìè **Analogizers** - Learn by recognizing similarity\n",
    "\n",
    "### What You'll Learn:\n",
    "1. How different ML paradigms approach the same problem\n",
    "2. The philosophical differences between approaches\n",
    "3. When to use each type of algorithm\n",
    "4. Working implementations you can modify and experiment with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Problem Setup](#problem-setup)\n",
    "3. [üå≥ Symbolists: Decision Trees](#symbolists)\n",
    "4. [üß† Connectionists: Neural Networks](#connectionists)\n",
    "5. [üß¨ Evolutionaries: Genetic Programming](#evolutionaries)\n",
    "6. [üìä Bayesians: Naive Bayes](#bayesians)\n",
    "7. [üìè Analogizers: k-Nearest Neighbors](#analogizers)\n",
    "8. [Comparison & Conclusion](#comparison)\n",
    "9. [Glossary](#glossary)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Configure Keras to use JAX backend (must be set before importing keras)\nimport os\nos.environ['KERAS_BACKEND'] = 'jax'\n\n# Standard library imports\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning - General\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\n\n# Machine learning - Tribe specific\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Neural networks - Keras 3.x with JAX backend\nimport keras\nfrom keras import layers\n\n# Genetic algorithms\nfrom deap import base, creator, tools, algorithms\nimport random\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\nkeras.utils.set_random_seed(42)\nrandom.seed(42)\n\n# Plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"All imports successful! ‚úì\")\nprint(f\"Using Keras backend: {keras.backend.backend()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"introduction\"></a>\n## Introduction\n\nMachine learning isn't just one thing‚Äîit's a collection of fundamentally different approaches to learning from data. Pedro Domingos, in his book \"The Master Algorithm,\" identifies five major \"tribes\" of machine learning, each with its own philosophy and techniques.\n\n**Why does this matter?** Because understanding these different paradigms helps you:\n- Choose the right algorithm for your problem\n- Understand why an algorithm works (or doesn't)\n- Combine approaches for better results\n- Think more deeply about what \"learning\" really means\n\nIn this notebook, we'll see how each tribe tackles the same problem: classifying iris flowers based on their physical measurements. By the end, you'll understand not just *that* different algorithms exist, but *why* they approach problems differently.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"problem-setup\"></a>\n## Problem Setup: The Iris Dataset\n\nThe Iris dataset is the \"Hello World\" of machine learning. It contains measurements of 150 iris flowers from three species:\n- **Setosa**\n- **Versicolor**\n- **Virginica**\n\nFor each flower, we have four measurements:\n1. Sepal length (cm)\n2. Sepal width (cm)\n3. Petal length (cm)\n4. Petal width (cm)\n\n**Our task:** Given these four measurements, predict which species the flower belongs to.\n\n**Why Iris?** It's simple enough to understand but complex enough to be non-trivial. Perfect for comparing different approaches!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\nfeature_names = iris.feature_names\ntarget_names = iris.target_names\n\n# Create a DataFrame for easier exploration\ndf = pd.DataFrame(X, columns=feature_names)\ndf['species'] = pd.Categorical.from_codes(y, target_names)\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Number of samples: {len(X)}\")\nprint(f\"Number of features: {X.shape[1]}\")\nprint(f\"Number of classes: {len(target_names)}\")\nprint(f\"\\nClass distribution:\")\nprint(df['species'].value_counts())\nprint(f\"\\nFirst 5 samples:\")\ndf.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize the data\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nfor idx, feature in enumerate(feature_names):\n    row = idx // 2\n    col = idx % 2\n    for species_idx, species in enumerate(target_names):\n        species_data = df[df['species'] == species][feature]\n        axes[row, col].hist(species_data, alpha=0.6, label=species, bins=15)\n    axes[row, col].set_xlabel(feature)\n    axes[row, col].set_ylabel('Frequency')\n    axes[row, col].legend()\n    axes[row, col].set_title(f'Distribution of {feature}')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üí° Notice how some features (like petal length) separate the species better than others!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Split into training and testing sets\n# We'll use the same split for all five tribes to ensure fair comparison\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set size: {len(X_train)} samples\")\nprint(f\"Test set size: {len(X_test)} samples\")\nprint(f\"\\nTraining set class distribution:\")\nprint(pd.Series(y_train).value_counts().sort_index())\nprint(f\"\\nTest set class distribution:\")\nprint(pd.Series(y_test).value_counts().sort_index())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"symbolists\"></a>\n## üå≥ Symbolists: Decision Trees\n\n### Philosophy\n\nSymbolists believe that learning is the **inverse of deduction**. Just as you can deduce specific conclusions from general rules, Symbolists learn by inducing general rules from specific examples.\n\n**Real-world analogy:** Think of how a detective works. They see clues (data) and build up a theory (rules) that explains all the evidence. \"If the footprint is larger than 12 inches AND the suspect is over 6 feet tall, THEN consider this person of interest.\"\n\n**Master Algorithm:** Inverse deduction\n\n### Key Concepts\n\n- **Logic and Rules**: Learning produces human-readable \"if-then\" rules\n- **Interpretability**: You can understand exactly why the algorithm made a decision\n- **Tree Structure**: Rules are organized hierarchically like a flowchart\n- **Greedy Splitting**: At each step, choose the split that best separates the classes\n- **Decision Boundaries**: Creates rectangular decision regions in feature space",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create and train a decision tree\ntree_model = DecisionTreeClassifier(\n    max_depth=3,  # Limit depth for interpretability\n    random_state=42\n)\n\ntree_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_tree = tree_model.predict(X_test)\n\n# Evaluate\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(f\"Decision Tree Accuracy: {accuracy_tree:.3f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_tree, target_names=target_names))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize the decision tree\nplt.figure(figsize=(20, 10))\nplot_tree(\n    tree_model,\n    feature_names=feature_names,\n    class_names=target_names,\n    filled=True,\n    rounded=True,\n    fontsize=10\n)\nplt.title(\"Decision Tree Structure\", fontsize=16, pad=20)\nplt.show()\n\n# Show the rules in text format\nprint(\"\\nüìã Decision Rules (text format):\")\nprint(\"=\"*50)\ntree_rules = export_text(tree_model, feature_names=feature_names)\nprint(tree_rules)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Results & Interpretation\n\nThe decision tree creates a flowchart of questions about the flowers' measurements. Notice how it:\n\n1. **Starts with the most informative feature** (usually petal-related measurements)\n2. **Creates simple yes/no questions** at each node\n3. **Produces human-readable rules** you could write down on paper\n\n**Strengths of the Symbolist approach:**\n- ‚úÖ Highly interpretable‚Äîyou can explain every decision\n- ‚úÖ No data preprocessing needed (no scaling required)\n- ‚úÖ Handles both numerical and categorical data\n- ‚úÖ Automatically does feature selection\n\n**Weaknesses:**\n- ‚ùå Can overfit if not constrained (tree too deep)\n- ‚ùå Unstable‚Äîsmall data changes can produce different trees\n- ‚ùå Creates axis-aligned boundaries (can't capture diagonal patterns well)\n\n**When to use:** When you need to explain your model's decisions to stakeholders, or when interpretability is crucial (medical diagnosis, loan approval, etc.)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"connectionists\"></a>\n## üß† Connectionists: Neural Networks\n\n### Philosophy\n\nConnectionists believe that intelligence emerges from **networks of simple units** working together, just like neurons in the brain. Learning happens by adjusting the connections between these units.\n\n**Real-world analogy:** Think of learning to ride a bike. You don't learn explicit rules‚Äîinstead, your brain's neural connections gradually adjust through practice until the right patterns emerge. You can't explain *how* you balance, but your brain knows.\n\n**Master Algorithm:** Backpropagation\n\n### Key Concepts\n\n- **Neurons and Layers**: Simple processing units organized in layers\n- **Weights and Biases**: Connections between neurons have adjustable strengths\n- **Activation Functions**: Non-linear transformations that enable complex patterns\n- **Gradient Descent**: Learning by following the slope of the error downhill\n- **Backpropagation**: Efficiently computing how to adjust weights to reduce error",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Neural networks work better with scaled data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"Data scaled for neural network training ‚úì\")\nprint(f\"Original range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\nprint(f\"Scaled range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Build a simple neural network\nnn_model = keras.Sequential([\n    layers.Dense(16, activation='relu', input_shape=(4,), name='hidden_layer_1'),\n    layers.Dense(8, activation='relu', name='hidden_layer_2'),\n    layers.Dense(3, activation='softmax', name='output_layer')\n])\n\n# Compile the model\nnn_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Display architecture\nprint(\"Neural Network Architecture:\")\nprint(\"=\"*50)\nnn_model.summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train the neural network\nhistory = nn_model.fit(\n    X_train_scaled, y_train,\n    epochs=100,\n    batch_size=16,\n    validation_split=0.2,\n    verbose=0\n)\n\n# Make predictions\ny_pred_probs = nn_model.predict(X_test_scaled, verbose=0)\ny_pred_nn = np.argmax(y_pred_probs, axis=1)\n\n# Evaluate\naccuracy_nn = accuracy_score(y_test, y_pred_nn)\nprint(f\"Neural Network Accuracy: {accuracy_nn:.3f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_nn, target_names=target_names))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize training history\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot accuracy\naxes[0].plot(history.history['accuracy'], label='Training Accuracy')\naxes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Model Accuracy Over Time')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot loss\naxes[1].plot(history.history['loss'], label='Training Loss')\naxes[1].plot(history.history['val_loss'], label='Validation Loss')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('Model Loss Over Time')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üí° Notice how the model learns: loss decreases and accuracy increases over epochs!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Results & Interpretation\n\nThe neural network learned by repeatedly adjusting its weights through backpropagation. Notice how:\n\n1. **Learning is gradual** - accuracy improves smoothly over epochs\n2. **It's a black box** - we can't easily explain individual decisions\n3. **It learns non-linear patterns** - the hidden layers capture complex relationships\n\n**Strengths of the Connectionist approach:**\n- ‚úÖ Can learn very complex, non-linear patterns\n- ‚úÖ Scales well to large datasets\n- ‚úÖ Can be extended (add more layers) for harder problems\n- ‚úÖ Works well with raw data (images, audio, text)\n\n**Weaknesses:**\n- ‚ùå Black box‚Äîhard to interpret why it made a decision\n- ‚ùå Requires lots of data to avoid overfitting\n- ‚ùå Needs careful tuning (learning rate, architecture, etc.)\n- ‚ùå Computationally expensive to train\n\n**When to use:** When you have lots of data, complex patterns to learn, and don't need to explain individual predictions (image recognition, speech recognition, etc.)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"evolutionaries\"></a>\n## üß¨ Evolutionaries: Genetic Algorithms\n\n### Philosophy\n\nEvolutionaries believe that learning is **simulated evolution**. Just as species evolve through natural selection, algorithms can evolve through mutation, crossover, and survival of the fittest.\n\n**Real-world analogy:** Think of breeding dogs. You start with a diverse population, select the best ones (fastest, strongest, friendliest), breed them to create offspring with mixed traits, and occasionally get random mutations. Over generations, the population gets better at whatever you're selecting for.\n\n**Master Algorithm:** Genetic programming\n\n### Key Concepts\n\n- **Population**: Multiple candidate solutions compete\n- **Fitness**: How well each candidate performs\n- **Selection**: Better candidates more likely to reproduce\n- **Crossover**: Combine two parents to create offspring\n- **Mutation**: Random changes to maintain diversity\n- **Evolution**: Populations improve over generations\n\n**Note:** For simplicity, we'll use genetic algorithms to optimize the parameters of a k-NN classifier rather than full genetic programming.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Set up DEAP for genetic algorithm\n# We'll evolve a simple classifier by optimizing feature weights\n\n# Create fitness and individual classes\ncreator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\ntoolbox = base.Toolbox()\n\n# Each gene is a feature weight between 0 and 1\ntoolbox.register(\"attr_float\", random.uniform, 0, 1)\ntoolbox.register(\"individual\", tools.initRepeat, creator.Individual,\n                 toolbox.attr_float, n=4)  # 4 features\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n\ndef eval_individual(individual, X_train, y_train, X_val, y_val):\n    \"\"\"Evaluate fitness by weighting features and testing accuracy\"\"\"\n    weights = np.array(individual)\n    \n    # Apply feature weights\n    X_train_weighted = X_train * weights\n    X_val_weighted = X_val * weights\n    \n    # Train simple k-NN classifier\n    knn = KNeighborsClassifier(n_neighbors=3)\n    knn.fit(X_train_weighted, y_train)\n    \n    # Return accuracy as fitness\n    accuracy = knn.score(X_val_weighted, y_val)\n    return (accuracy,)\n\n# Create validation split\nX_train_evo, X_val_evo, y_train_evo, y_val_evo = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\n\n# Register genetic operators\ntoolbox.register(\"evaluate\", eval_individual,\n                 X_train=X_train_evo, y_train=y_train_evo,\n                 X_val=X_val_evo, y_val=y_val_evo)\ntoolbox.register(\"mate\", tools.cxTwoPoint)\ntoolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.2)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\n\nprint(\"Genetic algorithm components initialized ‚úì\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run the genetic algorithm\npopulation_size = 50\nnum_generations = 40\n\n# Create initial population\npop = toolbox.population(n=population_size)\n\n# Track statistics\nfitness_over_time = []\nbest_fitness_over_time = []\n\nprint(\"Starting evolution...\")\nprint(\"=\"*50)\n\nfor gen in range(num_generations):\n    # Evaluate all individuals\n    fitnesses = list(map(toolbox.evaluate, pop))\n    for ind, fit in zip(pop, fitnesses):\n        ind.fitness.values = fit\n    \n    # Track progress\n    fits = [ind.fitness.values[0] for ind in pop]\n    fitness_over_time.append(np.mean(fits))\n    best_fitness_over_time.append(np.max(fits))\n    \n    if gen % 10 == 0:\n        print(f\"Generation {gen}: Avg Fitness = {np.mean(fits):.3f}, \"\n              f\"Best Fitness = {np.max(fits):.3f}\")\n    \n    # Select and breed next generation\n    offspring = toolbox.select(pop, len(pop))\n    offspring = list(map(toolbox.clone, offspring))\n    \n    # Apply crossover\n    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n        if random.random() < 0.7:\n            toolbox.mate(child1, child2)\n            del child1.fitness.values\n            del child2.fitness.values\n    \n    # Apply mutation\n    for mutant in offspring:\n        if random.random() < 0.2:\n            toolbox.mutate(mutant)\n            del mutant.fitness.values\n    \n    pop[:] = offspring\n\n# Get best individual\nbest_ind = tools.selBest(pop, 1)[0]\nbest_weights = np.array(best_ind)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Evolution complete!\")\nprint(f\"Best feature weights: {best_weights}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test the evolved solution\nX_test_weighted = X_test * best_weights\n\n# Train final model with evolved weights\nknn_evo = KNeighborsClassifier(n_neighbors=3)\nknn_evo.fit(X_train * best_weights, y_train)\n\n# Predict\ny_pred_evo = knn_evo.predict(X_test_weighted)\n\n# Evaluate\naccuracy_evo = accuracy_score(y_test, y_pred_evo)\nprint(f\"Genetic Algorithm Accuracy: {accuracy_evo:.3f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_evo, target_names=target_names))\n\nprint(f\"\\nüí° Feature importance (evolved weights):\")\nfor feature, weight in zip(feature_names, best_weights):\n    print(f\"  {feature}: {weight:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize evolution progress\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot fitness over generations\naxes[0].plot(fitness_over_time, label='Average Fitness', linewidth=2)\naxes[0].plot(best_fitness_over_time, label='Best Fitness', linewidth=2)\naxes[0].set_xlabel('Generation')\naxes[0].set_ylabel('Fitness (Accuracy)')\naxes[0].set_title('Evolution of Population Fitness')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot feature weights\naxes[1].bar(range(len(feature_names)), best_weights, color='green', alpha=0.7)\naxes[1].set_xticks(range(len(feature_names)))\naxes[1].set_xticklabels(feature_names, rotation=45, ha='right')\naxes[1].set_ylabel('Weight')\naxes[1].set_title('Evolved Feature Weights')\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üí° Notice how the population improves over generations through selection and variation!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Results & Interpretation\n\nThe genetic algorithm evolved feature weights through natural selection. Notice how:\n\n1. **Fitness improves over generations** - the population adapts to the problem\n2. **Diversity matters** - mutation prevents premature convergence\n3. **No gradients needed** - works even when we can't compute derivatives\n\n**Strengths of the Evolutionary approach:**\n- ‚úÖ Works on any fitness function (no need for gradients)\n- ‚úÖ Can optimize discrete or continuous parameters\n- ‚úÖ Good at avoiding local optima (thanks to diversity)\n- ‚úÖ Naturally parallelizable (evaluate population in parallel)\n\n**Weaknesses:**\n- ‚ùå Computationally expensive (many fitness evaluations)\n- ‚ùå Slow convergence compared to gradient-based methods\n- ‚ùå Many hyperparameters to tune (population size, mutation rate, etc.)\n- ‚ùå No guarantees of finding global optimum\n\n**When to use:** When you can't compute gradients, have a complex search space, or need to optimize discrete structures (network architectures, rule sets, etc.)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"bayesians\"></a>\n## üìä Bayesians: Naive Bayes Classifier\n\n### Philosophy\n\nBayesians believe that learning is a form of **probabilistic inference**. All learning is about updating your beliefs based on evidence using Bayes' theorem.\n\n**Real-world analogy:** Think of a doctor diagnosing a patient. They start with prior knowledge (how common is this disease?), observe symptoms (evidence), and update their belief about what the patient has. More evidence = more confident diagnosis.\n\n**Master Algorithm:** Bayesian inference\n\n### Key Concepts\n\n- **Prior Probability**: What we believe before seeing evidence\n- **Likelihood**: How probable is this evidence given the hypothesis?\n- **Posterior Probability**: Updated belief after seeing evidence\n- **Bayes' Theorem**: P(Hypothesis|Evidence) = P(Evidence|Hypothesis) √ó P(Hypothesis) / P(Evidence)\n- **Naive Assumption**: Features are independent (simplifies calculation)\n\n**Formula:**\n```\nP(Species|Measurements) ‚àù P(Measurements|Species) √ó P(Species)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create and train Naive Bayes classifier\nnb_model = GaussianNB()\n\nnb_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_nb = nb_model.predict(X_test)\ny_pred_proba_nb = nb_model.predict_proba(X_test)\n\n# Evaluate\naccuracy_nb = accuracy_score(y_test, y_pred_nb)\nprint(f\"Naive Bayes Accuracy: {accuracy_nb:.3f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_nb, target_names=target_names))\n\n# Show example predictions with probabilities\nprint(\"\\nüìä Example predictions with probabilities:\")\nprint(\"=\"*60)\nfor i in range(5):\n    true_class = target_names[y_test[i]]\n    pred_class = target_names[y_pred_nb[i]]\n    probs = y_pred_proba_nb[i]\n    print(f\"Sample {i+1}: True={true_class}, Predicted={pred_class}\")\n    for j, species in enumerate(target_names):\n        print(f\"  P({species}|measurements) = {probs[j]:.3f}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize probability distributions for each class\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\nfor idx, feature in enumerate(feature_names):\n    row = idx // 2\n    col = idx % 2\n    \n    # For each class, plot the learned Gaussian distribution\n    for class_idx in range(3):\n        # Get mean and variance for this feature and class\n        mean = nb_model.theta_[class_idx, idx]\n        var = nb_model.var_[class_idx, idx]\n        std = np.sqrt(var)\n        \n        # Generate distribution curve\n        x_range = np.linspace(mean - 3*std, mean + 3*std, 100)\n        gaussian = (1 / (std * np.sqrt(2 * np.pi))) * \\\n                   np.exp(-0.5 * ((x_range - mean) / std) ** 2)\n        \n        axes[row, col].plot(x_range, gaussian, label=target_names[class_idx],\n                           linewidth=2)\n    \n    axes[row, col].set_xlabel(feature)\n    axes[row, col].set_ylabel('Probability Density')\n    axes[row, col].set_title(f'Learned Distributions for {feature}')\n    axes[row, col].legend()\n    axes[row, col].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üí° The classifier learned the probability distribution of each feature for each species!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show prior and class probabilities\nprint(\"üìä Prior Probabilities (from training data):\")\nprint(\"=\"*50)\nfor idx, species in enumerate(target_names):\n    prior = nb_model.class_prior_[idx]\n    count = nb_model.class_count_[idx]\n    print(f\"{species:12} : {prior:.3f} ({int(count)} samples)\")\n\n# Visualize\nplt.figure(figsize=(8, 5))\nplt.bar(target_names, nb_model.class_prior_, color=['blue', 'orange', 'green'], alpha=0.7)\nplt.ylabel('Prior Probability')\nplt.title('Prior Probabilities of Each Species')\nplt.grid(True, alpha=0.3, axis='y')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Results & Interpretation\n\nThe Naive Bayes classifier learned probability distributions for each feature. When classifying a new flower, it:\n\n1. **Starts with priors** - how common is each species in the training data?\n2. **Observes evidence** - what are this flower's measurements?\n3. **Computes likelihoods** - how probable are these measurements for each species?\n4. **Updates beliefs** - use Bayes' theorem to get posterior probabilities\n5. **Makes decision** - pick the species with highest probability\n\n**Strengths of the Bayesian approach:**\n- ‚úÖ Provides probability estimates, not just predictions\n- ‚úÖ Works well with small datasets\n- ‚úÖ Fast training and prediction\n- ‚úÖ Handles missing data gracefully\n- ‚úÖ Theoretically principled (based on probability theory)\n\n**Weaknesses:**\n- ‚ùå \"Naive\" independence assumption often violated\n- ‚ùå Probability estimates can be overconfident\n- ‚ùå Sensitive to how you model the distributions\n- ‚ùå Can struggle with correlated features\n\n**When to use:** When you need probability estimates (not just classifications), have limited data, or want a fast baseline. Great for text classification, spam filtering, and medical diagnosis.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"analogizers\"></a>\n## üìè Analogizers: k-Nearest Neighbors\n\n### Philosophy\n\nAnalogizers believe that the key to learning is **recognizing similarity**. To classify something new, find similar examples you've seen before and predict based on them.\n\n**Real-world analogy:** Think of how you identify mushrooms in the forest. You don't have explicit rules or probability distributions‚Äîyou compare the mushroom you found to ones you've seen before. \"This one looks like the edible mushrooms I know, so it's probably safe.\"\n\n**Master Algorithm:** Support Vector Machine (but we'll use k-NN for simplicity)\n\n### Key Concepts\n\n- **Similarity/Distance**: Measure how \"close\" examples are to each other\n- **No Training Phase**: Just memorize all the examples\n- **Lazy Learning**: All the work happens at prediction time\n- **k Neighbors**: Look at the k most similar examples\n- **Majority Vote**: Predict the most common class among neighbors\n- **Distance Metric**: Usually Euclidean distance in feature space",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create and train k-NN classifier\n# Try different values of k\nk_values = [1, 3, 5, 7]\nknn_models = {}\nknn_accuracies = {}\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    knn_models[k] = knn\n    knn_accuracies[k] = accuracy\n    \n    print(f\"k={k}: Accuracy = {accuracy:.3f}\")\n\n# Use k=3 as our primary model\nbest_k = max(knn_accuracies, key=knn_accuracies.get)\nknn_model = knn_models[best_k]\ny_pred_knn = knn_model.predict(X_test)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Best k: {best_k}\")\nprint(f\"k-NN Accuracy: {knn_accuracies[best_k]:.3f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_knn, target_names=target_names))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize how k-NN works for a test sample\ntest_idx = 0\ntest_sample = X_test[test_idx].reshape(1, -1)\ntrue_label = y_test[test_idx]\n\n# Find nearest neighbors\ndistances, indices = knn_model.kneighbors(test_sample, n_neighbors=5)\n\nprint(f\"Example: Classifying test sample {test_idx}\")\nprint(f\"True class: {target_names[true_label]}\")\nprint(f\"Predicted class: {target_names[y_pred_knn[test_idx]]}\")\nprint(f\"\\nüìè Nearest {best_k} neighbors:\")\nprint(\"=\"*60)\n\nfor i, (dist, idx) in enumerate(zip(distances[0][:best_k], indices[0][:best_k])):\n    neighbor_class = y_train[idx]\n    print(f\"Neighbor {i+1}: Distance={dist:.3f}, Class={target_names[neighbor_class]}\")\n\n# Visualize in 2D (using first two features)\nplt.figure(figsize=(10, 6))\n\n# Plot training data\nfor class_idx, species in enumerate(target_names):\n    mask = y_train == class_idx\n    plt.scatter(X_train[mask, 0], X_train[mask, 1],\n               label=species, alpha=0.5, s=50)\n\n# Plot test sample\nplt.scatter(test_sample[0, 0], test_sample[0, 1],\n           color='red', marker='*', s=500,\n           edgecolors='black', linewidth=2,\n           label='Test Sample', zorder=5)\n\n# Plot nearest neighbors\nneighbor_points = X_train[indices[0][:best_k]]\nplt.scatter(neighbor_points[:, 0], neighbor_points[:, 1],\n           color='black', marker='o', s=200,\n           facecolors='none', linewidth=2,\n           label=f'{best_k} Nearest Neighbors', zorder=4)\n\n# Draw lines to neighbors\nfor neighbor in neighbor_points:\n    plt.plot([test_sample[0, 0], neighbor[0]],\n            [test_sample[0, 1], neighbor[1]],\n            'k--', alpha=0.3, linewidth=1)\n\nplt.xlabel(feature_names[0])\nplt.ylabel(feature_names[1])\nplt.title(f'k-NN Classification (k={best_k}) - Using First Two Features')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\\nüí° The test sample is classified based on its nearest neighbors!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize decision boundaries (using petal length and width)\n# These features separate the classes best\nfeature_idx = [2, 3]  # Petal length and width\n\n# Create mesh\nh = 0.02  # step size in mesh\nx_min = X[:, feature_idx[0]].min() - 0.5\nx_max = X[:, feature_idx[0]].max() + 0.5\ny_min = X[:, feature_idx[1]].min() - 0.5\ny_max = X[:, feature_idx[1]].max() + 0.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Train k-NN on just these two features\nknn_2d = KNeighborsClassifier(n_neighbors=best_k)\nknn_2d.fit(X_train[:, feature_idx], y_train)\n\n# Predict over mesh\nZ = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\nplt.colorbar(ticks=[0, 1, 2], label='Species')\n\n# Plot training points\ncolors = ['blue', 'orange', 'green']\nfor class_idx, species in enumerate(target_names):\n    mask = y_train == class_idx\n    plt.scatter(X_train[mask, feature_idx[0]],\n               X_train[mask, feature_idx[1]],\n               c=colors[class_idx], label=species,\n               edgecolors='black', s=50)\n\nplt.xlabel(feature_names[feature_idx[0]])\nplt.ylabel(feature_names[feature_idx[1]])\nplt.title(f'k-NN Decision Boundaries (k={best_k})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"üí° Notice the smooth, organic boundaries - each region contains points similar to that class!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Results & Interpretation\n\nk-NN is beautifully simple: to classify something, find similar examples and copy their labels. Notice how:\n\n1. **No \"training\"** - just memorize the data\n2. **Prediction is local** - only nearby points matter\n3. **Decision boundaries are smooth** - follow the natural clustering of data\n4. **k matters** - small k is sensitive to noise, large k is smooth but less precise\n\n**Strengths of the Analogizer approach:**\n- ‚úÖ Extremely simple to understand and implement\n- ‚úÖ No assumptions about data distribution\n- ‚úÖ Naturally handles multi-class problems\n- ‚úÖ Can adapt to any decision boundary shape\n- ‚úÖ Easy to update (just add new examples)\n\n**Weaknesses:**\n- ‚ùå Slow prediction (must search all training data)\n- ‚ùå Memory intensive (stores all training data)\n- ‚ùå Sensitive to irrelevant features and scale\n- ‚ùå Struggles in high dimensions (\"curse of dimensionality\")\n\n**When to use:** When you have moderate-sized datasets, need to explain predictions by example, or want a simple baseline. Great for recommendation systems, image classification, and pattern recognition.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"comparison\"></a>\n## Comparison & Conclusion\n\nNow that we've seen all five tribes in action, let's compare their performance and understand when to use each approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Collect all results\nresults = {\n    'Symbolists (Decision Tree)': accuracy_tree,\n    'Connectionists (Neural Net)': accuracy_nn,\n    'Evolutionaries (Genetic Alg)': accuracy_evo,\n    'Bayesians (Naive Bayes)': accuracy_nb,\n    'Analogizers (k-NN)': knn_accuracies[best_k]\n}\n\n# Create comparison DataFrame\nresults_df = pd.DataFrame({\n    'Tribe': list(results.keys()),\n    'Accuracy': list(results.values())\n}).sort_values('Accuracy', ascending=False)\n\nprint(\"üìä Performance Comparison\")\nprint(\"=\"*60)\nprint(results_df.to_string(index=False))\n\n# Visualize\nplt.figure(figsize=(12, 6))\nbars = plt.bar(range(len(results)), results.values(),\n               color=['forestgreen', 'navy', 'purple', 'orange', 'crimson'],\n               alpha=0.7, edgecolor='black', linewidth=2)\nplt.xticks(range(len(results)), results.keys(), rotation=15, ha='right')\nplt.ylabel('Accuracy')\nplt.title('Five Tribes Performance on Iris Classification')\nplt.ylim([0.9, 1.0])  # Focus on differences\nplt.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor i, (tribe, acc) in enumerate(results.items()):\n    plt.text(i, acc + 0.003, f'{acc:.3f}', ha='center', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare confusion matrices\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\npredictions = [\n    ('Decision Tree', y_pred_tree),\n    ('Neural Network', y_pred_nn),\n    ('Genetic Algorithm', y_pred_evo),\n    ('Naive Bayes', y_pred_nb),\n    ('k-NN', y_pred_knn)\n]\n\nfor idx, (name, y_pred) in enumerate(predictions):\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=target_names, yticklabels=target_names,\n               ax=axes[idx], cbar=False)\n    axes[idx].set_title(name)\n    axes[idx].set_ylabel('True Label')\n    axes[idx].set_xlabel('Predicted Label')\n\n# Hide the extra subplot\naxes[5].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üí° Notice which species each tribe struggles with (if any)!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### When to Use Each Tribe\n\n| Tribe | Best For | Avoid When |\n|-------|----------|------------|\n| üå≥ **Symbolists** | Need interpretability, explaining decisions to stakeholders, moderate data | Very large datasets, need high accuracy on complex patterns |\n| üß† **Connectionists** | Large datasets, complex patterns (images, audio, text), raw data | Need to explain decisions, small data, limited compute |\n| üß¨ **Evolutionaries** | Can't compute gradients, discrete optimization, complex search spaces | Time-critical applications, need fast convergence |\n| üìä **Bayesians** | Need probability estimates, small data, handling uncertainty | Features are highly correlated, need precise probabilities |\n| üìè **Analogizers** | Need to explain by example, moderate data, multi-class problems | Very large datasets, high-dimensional data, need speed |\n\n### Key Takeaways\n\n1. **No single best approach** - each tribe excels in different situations\n2. **They complement each other** - combining tribes can work even better\n3. **Philosophy matters** - understanding *why* an algorithm works helps you use it well\n4. **The Master Algorithm** - Domingos envisions combining all five approaches into one\n\n### Toward the Master Algorithm\n\nThe \"holy grail\" of machine learning is a single algorithm that combines the strengths of all five tribes:\n- **Symbolists**: Interpretability and logical structure\n- **Connectionists**: Learning complex representations\n- **Evolutionaries**: Optimization without gradients\n- **Bayesians**: Handling uncertainty\n- **Analogizers**: Recognizing similarity\n\nSome modern approaches are already doing this:\n- **Probabilistic programming** (Bayesian + Symbolist)\n- **Neural-symbolic AI** (Connectionist + Symbolist)\n- **AutoML** (Evolutionary + any tribe)\n- **Ensemble methods** (combining multiple tribes)\n\nThe quest continues!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"glossary\"></a>\n## Glossary\n\n**Accuracy**: Percentage of correct predictions\n\n**Activation Function**: Non-linear transformation in neural networks (ReLU, sigmoid, etc.)\n\n**Backpropagation**: Algorithm for training neural networks by computing gradients\n\n**Bayes' Theorem**: Formula for updating probabilities based on evidence\n\n**Classification**: Predicting which category something belongs to\n\n**Confusion Matrix**: Table showing correct and incorrect predictions by class\n\n**Crossover**: Genetic algorithm operation that combines two parents\n\n**Decision Boundary**: Line/surface separating different classes in feature space\n\n**Epoch**: One complete pass through the training data\n\n**Feature**: Measurable property used for prediction (e.g., petal length)\n\n**Fitness**: How well a genetic algorithm individual solves the problem\n\n**Gradient Descent**: Optimization by following the slope downhill\n\n**k-Nearest Neighbors (k-NN)**: Classify based on k most similar training examples\n\n**Mutation**: Random change in genetic algorithms\n\n**Neural Network**: Model inspired by brain structure, with layers of connected neurons\n\n**Overfitting**: Model learns training data too well, fails on new data\n\n**Posterior Probability**: Updated belief after seeing evidence (Bayesian)\n\n**Prior Probability**: Initial belief before seeing evidence (Bayesian)\n\n**Test Set**: Data held out for final evaluation\n\n**Training Set**: Data used to teach the model\n\n**Validation Set**: Data used to tune hyperparameters during training",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Further Reading\n\n### The Book\n- **\"The Master Algorithm\"** by Pedro Domingos - the inspiration for this notebook\n\n### Learn More\n- [Scikit-learn Documentation](https://scikit-learn.org/) - ML library we used\n- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials) - Deep learning\n- [DEAP Documentation](https://deap.readthedocs.io/) - Genetic algorithms\n\n### Next Steps\n1. **Experiment**: Try changing parameters and see what happens\n2. **New datasets**: Apply these approaches to different problems\n3. **Combine tribes**: Experiment with ensemble methods\n4. **Deep dive**: Pick your favorite tribe and study it deeply\n\n---\n\n**Thank you for exploring the five tribes of machine learning!**\n\n*\"The grand aim of science is to cover the greatest number of experimental facts by logical deduction from the smallest number of hypotheses or axioms.\" - Albert Einstein*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}