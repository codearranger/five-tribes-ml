{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Tribes of Machine Learning: Iris Classification\n",
    "## An Educational Demonstration\n",
    "\n",
    "Welcome! This notebook demonstrates how the five tribes of machine learning (from Pedro Domingos' book \"The Master Algorithm\") each approach the classic Iris flower classification problem.\n",
    "\n",
    "### The Five Tribes:\n",
    "- üå≥ **Symbolists** - Learn through logical rules\n",
    "- üß† **Connectionists** - Learn by mimicking the brain\n",
    "- üß¨ **Evolutionaries** - Learn through simulated evolution\n",
    "- üìä **Bayesians** - Learn through probabilistic inference\n",
    "- üìè **Analogizers** - Learn by recognizing similarity\n",
    "\n",
    "### What You'll Learn:\n",
    "1. How different ML paradigms approach the same problem\n",
    "2. The philosophical differences between approaches\n",
    "3. When to use each type of algorithm\n",
    "4. Working implementations you can modify and experiment with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Problem Setup](#problem-setup)\n",
    "3. [üå≥ Symbolists: Decision Trees](#symbolists)\n",
    "4. [üß† Connectionists: Neural Networks](#connectionists)\n",
    "5. [üß¨ Evolutionaries: Genetic Programming](#evolutionaries)\n",
    "6. [üìä Bayesians: Naive Bayes](#bayesians)\n",
    "7. [üìè Analogizers: k-Nearest Neighbors](#analogizers)\n",
    "8. [Comparison & Conclusion](#comparison)\n",
    "9. [Glossary](#glossary)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Configure Keras to use JAX backend (must be set before importing keras)\nimport os\nos.environ['KERAS_BACKEND'] = 'jax'\n\n# Standard library imports\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning - General\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\n\n# Machine learning - Tribe specific\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Neural networks - Keras 3.x with JAX backend\nimport keras\nfrom keras import layers\n\n# Genetic algorithms\nfrom deap import base, creator, tools, algorithms\nimport random\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\nkeras.utils.set_random_seed(42)\nrandom.seed(42)\n\n# Plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"All imports successful! ‚úì\")\nprint(f\"Using Keras backend: {keras.backend.backend()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"introduction\"></a>\n## Introduction\n\nMachine learning isn't just one thing‚Äîit's a collection of fundamentally different approaches to learning from data. Pedro Domingos, in his book \"The Master Algorithm,\" identifies five major \"tribes\" of machine learning, each with its own philosophy and techniques.\n\n**Why does this matter?** Because understanding these different paradigms helps you:\n- Choose the right algorithm for your problem\n- Understand why an algorithm works (or doesn't)\n- Combine approaches for better results\n- Think more deeply about what \"learning\" really means\n\nIn this notebook, we'll see how each tribe tackles the same problem: classifying iris flowers based on their physical measurements. By the end, you'll understand not just *that* different algorithms exist, but *why* they approach problems differently.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"problem-setup\"></a>\n## Problem Setup: The Iris Dataset\n\nThe Iris dataset is the \"Hello World\" of machine learning. It contains measurements of 150 iris flowers from three species:\n- **Setosa**\n- **Versicolor**\n- **Virginica**\n\nFor each flower, we have four measurements:\n1. Sepal length (cm)\n2. Sepal width (cm)\n3. Petal length (cm)\n4. Petal width (cm)\n\n**Our task:** Given these four measurements, predict which species the flower belongs to.\n\n**Why Iris?** It's simple enough to understand but complex enough to be non-trivial. Perfect for comparing different approaches!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\nfeature_names = iris.feature_names\ntarget_names = iris.target_names\n\n# Create a DataFrame for easier exploration\ndf = pd.DataFrame(X, columns=feature_names)\ndf['species'] = pd.Categorical.from_codes(y, target_names)\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Number of samples: {len(X)}\")\nprint(f\"Number of features: {X.shape[1]}\")\nprint(f\"Number of classes: {len(target_names)}\")\nprint(f\"\\nClass distribution:\")\nprint(df['species'].value_counts())\nprint(f\"\\nFirst 5 samples:\")\ndf.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize the data\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nfor idx, feature in enumerate(feature_names):\n    row = idx // 2\n    col = idx % 2\n    for species_idx, species in enumerate(target_names):\n        species_data = df[df['species'] == species][feature]\n        axes[row, col].hist(species_data, alpha=0.6, label=species, bins=15)\n    axes[row, col].set_xlabel(feature)\n    axes[row, col].set_ylabel('Frequency')\n    axes[row, col].legend()\n    axes[row, col].set_title(f'Distribution of {feature}')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üí° Notice how some features (like petal length) separate the species better than others!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Split into training and testing sets\n# We'll use the same split for all five tribes to ensure fair comparison\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set size: {len(X_train)} samples\")\nprint(f\"Test set size: {len(X_test)} samples\")\nprint(f\"\\nTraining set class distribution:\")\nprint(pd.Series(y_train).value_counts().sort_index())\nprint(f\"\\nTest set class distribution:\")\nprint(pd.Series(y_test).value_counts().sort_index())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"symbolists\"></a>\n## üå≥ Symbolists: Decision Trees\n\n### Philosophy\n\nSymbolists believe that learning is the **inverse of deduction**. Just as you can deduce specific conclusions from general rules, Symbolists learn by inducing general rules from specific examples.\n\n**Real-world analogy:** Think of how a detective works. They see clues (data) and build up a theory (rules) that explains all the evidence. \"If the footprint is larger than 12 inches AND the suspect is over 6 feet tall, THEN consider this person of interest.\"\n\n**Master Algorithm:** Inverse deduction\n\n### Key Concepts\n\n- **Logic and Rules**: Learning produces human-readable \"if-then\" rules\n- **Interpretability**: You can understand exactly why the algorithm made a decision\n- **Tree Structure**: Rules are organized hierarchically like a flowchart\n- **Greedy Splitting**: At each step, choose the split that best separates the classes\n- **Decision Boundaries**: Creates rectangular decision regions in feature space",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create and train a decision tree\ntree_model = DecisionTreeClassifier(\n    max_depth=3,  # Limit depth for interpretability\n    random_state=42\n)\n\ntree_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_tree = tree_model.predict(X_test)\n\n# Evaluate\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(f\"Decision Tree Accuracy: {accuracy_tree:.3f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_tree, target_names=target_names))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize the decision tree\nplt.figure(figsize=(20, 10))\nplot_tree(\n    tree_model,\n    feature_names=feature_names,\n    class_names=target_names,\n    filled=True,\n    rounded=True,\n    fontsize=10\n)\nplt.title(\"Decision Tree Structure\", fontsize=16, pad=20)\nplt.show()\n\n# Show the rules in text format\nprint(\"\\nüìã Decision Rules (text format):\")\nprint(\"=\"*50)\ntree_rules = export_text(tree_model, feature_names=feature_names)\nprint(tree_rules)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Results & Interpretation\n\nThe decision tree creates a flowchart of questions about the flowers' measurements. Notice how it:\n\n1. **Starts with the most informative feature** (usually petal-related measurements)\n2. **Creates simple yes/no questions** at each node\n3. **Produces human-readable rules** you could write down on paper\n\n**Strengths of the Symbolist approach:**\n- ‚úÖ Highly interpretable‚Äîyou can explain every decision\n- ‚úÖ No data preprocessing needed (no scaling required)\n- ‚úÖ Handles both numerical and categorical data\n- ‚úÖ Automatically does feature selection\n\n**Weaknesses:**\n- ‚ùå Can overfit if not constrained (tree too deep)\n- ‚ùå Unstable‚Äîsmall data changes can produce different trees\n- ‚ùå Creates axis-aligned boundaries (can't capture diagonal patterns well)\n\n**When to use:** When you need to explain your model's decisions to stakeholders, or when interpretability is crucial (medical diagnosis, loan approval, etc.)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"connectionists\"></a>\n## üß† Connectionists: Neural Networks\n\n### Philosophy\n\nConnectionists believe that intelligence emerges from **networks of simple units** working together, just like neurons in the brain. Learning happens by adjusting the connections between these units.\n\n**Real-world analogy:** Think of learning to ride a bike. You don't learn explicit rules‚Äîinstead, your brain's neural connections gradually adjust through practice until the right patterns emerge. You can't explain *how* you balance, but your brain knows.\n\n**Master Algorithm:** Backpropagation\n\n### Key Concepts\n\n- **Neurons and Layers**: Simple processing units organized in layers\n- **Weights and Biases**: Connections between neurons have adjustable strengths\n- **Activation Functions**: Non-linear transformations that enable complex patterns\n- **Gradient Descent**: Learning by following the slope of the error downhill\n- **Backpropagation**: Efficiently computing how to adjust weights to reduce error",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Neural networks work better with scaled data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"Data scaled for neural network training ‚úì\")\nprint(f\"Original range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\nprint(f\"Scaled range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Build a simple neural network\nnn_model = keras.Sequential([\n    layers.Dense(16, activation='relu', input_shape=(4,), name='hidden_layer_1'),\n    layers.Dense(8, activation='relu', name='hidden_layer_2'),\n    layers.Dense(3, activation='softmax', name='output_layer')\n])\n\n# Compile the model\nnn_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Display architecture\nprint(\"Neural Network Architecture:\")\nprint(\"=\"*50)\nnn_model.summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train the neural network\nhistory = nn_model.fit(\n    X_train_scaled, y_train,\n    epochs=100,\n    batch_size=16,\n    validation_split=0.2,\n    verbose=0\n)\n\n# Make predictions\ny_pred_probs = nn_model.predict(X_test_scaled, verbose=0)\ny_pred_nn = np.argmax(y_pred_probs, axis=1)\n\n# Evaluate\naccuracy_nn = accuracy_score(y_test, y_pred_nn)\nprint(f\"Neural Network Accuracy: {accuracy_nn:.3f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_nn, target_names=target_names))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize training history\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot accuracy\naxes[0].plot(history.history['accuracy'], label='Training Accuracy')\naxes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Model Accuracy Over Time')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot loss\naxes[1].plot(history.history['loss'], label='Training Loss')\naxes[1].plot(history.history['val_loss'], label='Validation Loss')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('Model Loss Over Time')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üí° Notice how the model learns: loss decreases and accuracy increases over epochs!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Results & Interpretation\n\nThe neural network learned by repeatedly adjusting its weights through backpropagation. Notice how:\n\n1. **Learning is gradual** - accuracy improves smoothly over epochs\n2. **It's a black box** - we can't easily explain individual decisions\n3. **It learns non-linear patterns** - the hidden layers capture complex relationships\n\n**Strengths of the Connectionist approach:**\n- ‚úÖ Can learn very complex, non-linear patterns\n- ‚úÖ Scales well to large datasets\n- ‚úÖ Can be extended (add more layers) for harder problems\n- ‚úÖ Works well with raw data (images, audio, text)\n\n**Weaknesses:**\n- ‚ùå Black box‚Äîhard to interpret why it made a decision\n- ‚ùå Requires lots of data to avoid overfitting\n- ‚ùå Needs careful tuning (learning rate, architecture, etc.)\n- ‚ùå Computationally expensive to train\n\n**When to use:** When you have lots of data, complex patterns to learn, and don't need to explain individual predictions (image recognition, speech recognition, etc.)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}